{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Keras layers for LSTM RNN \n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, TimeDistributed, Flatten\n",
    "from keras import regularizers, optimizers\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "## load data\n",
    "print('Loading data...')\n",
    "X = np.genfromtxt('shuffledSet_w50_s10.txt')\n",
    "X1 = np.reshape(X,(8430,50,11))\n",
    "\n",
    "## ========================   headers  ====================================================\n",
    "# Depth    ROP     WOB     RPM     Torque     Bit Size     GR     RHOB     NPHI     DT     RD\n",
    "## ========================================================================================\n",
    "\n",
    "print('data loaded')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6400, 50, 3) (6400, 50) (1600, 50, 3) (1600, 50)\n"
     ]
    }
   ],
   "source": [
    "# split to test and train\n",
    "X, Y = X1[:,:,0:6], X1[:,:,6:]\n",
    "\n",
    "# split into train and test sets\n",
    "x_train = X[:6400,:,[1, 2, 4]]\n",
    "y_train = Y[:6400, :, 1]  # 0 for GR, 1 for RHOB\n",
    "\n",
    "x_test = X[-1600:,:,[1, 2, 4]]\n",
    "y_test = Y[-1600:, :, 1]\n",
    "\n",
    "\n",
    "## sanity check\n",
    "#x_train = X[0:1,:,:]\n",
    "#y_train = Y[0:1, :, 1]  # 0 for GR, 1 for RHOB\n",
    "\n",
    "#x_test = X[6744:,:,:]\n",
    "#y_test = Y[6744:, :, 1]\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set is uploaded and sliced as follows, [samples, timesteps, features] = $[m, T_x, x]$\n",
    "\n",
    "Then we implement our BiLSTM model.\n",
    "\n",
    "The model has the following architecture. ==>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wavelet transform for later\n",
    "#x = np.asarray(x_train[0,:,1])\n",
    "#y = np.asarray(y_train[0,:])\n",
    "#print(x.shape)\n",
    "#cA, cD, cB, cC = np.asarray(pywt.wavedec(x, 'haar', mode='constant', level=3))\n",
    "#aA, aD, aB, aC = np.asarray(pywt.wavedec(y, 'haar', mode='constant', level=3))\n",
    "\n",
    "#plt.plot(aD)\n",
    "#plt.plot(cD)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StatefulLSTM(x_train, y_train, batch_size, units, dropout, epochNo, learning_rate, x_test, y_test):\n",
    "    \n",
    "    # network\n",
    "    input_dim = (x_train.shape[1],x_train.shape[2])\n",
    "    print((input_dim))\n",
    "    \n",
    "    new_model = Sequential()\n",
    "    model = Sequential()\n",
    "    for i in range(len(units)-1):\n",
    "        #print((units[i]))\n",
    "        model.add(LSTM(units[i], batch_input_shape = (batch_size, input_dim[0], input_dim[1]), \n",
    "                       kernel_regularizer=regularizers.l2(0.01), \n",
    "                       recurrent_regularizer=regularizers.l2(0.01), \n",
    "                       bias_regularizer=regularizers.l2(0.01), \n",
    "                       activation='tanh', \n",
    "                        stateful=True, return_sequences=True))\n",
    "        \n",
    "        new_model.add(LSTM(units[i], batch_input_shape = (1, input_dim[0], input_dim[1]), \n",
    "                       kernel_regularizer=regularizers.l2(0.01), \n",
    "                       recurrent_regularizer=regularizers.l2(0.01), \n",
    "                       bias_regularizer=regularizers.l2(0.01), \n",
    "                       activation='tanh', \n",
    "                        stateful=True, return_sequences=True))\n",
    "        \n",
    "        new_model.add(Dropout(dropout))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "\n",
    "    model.add(LSTM(units[len(units)-1], batch_input_shape = (batch_size, input_dim[0], input_dim[1]),\n",
    "                   kernel_regularizer=regularizers.l2(0.01), \n",
    "                   recurrent_regularizer=regularizers.l2(0.01), \n",
    "                   bias_regularizer=regularizers.l2(0.01), \n",
    "                   activation='tanh', stateful=True))\n",
    "\n",
    "    new_model.add(LSTM(units[len(units)-1], batch_input_shape = (1, input_dim[0], input_dim[1]),\n",
    "                   kernel_regularizer=regularizers.l2(0.01), \n",
    "                   recurrent_regularizer=regularizers.l2(0.01), \n",
    "                   bias_regularizer=regularizers.l2(0.01), \n",
    "                   activation='tanh', stateful=True))\n",
    "\n",
    "    \n",
    "    \n",
    "    model.add(Dropout(dropout))\n",
    "    model.add((Dense(input_dim[0])))\n",
    "\n",
    "    \n",
    "    new_model.add(Dropout(dropout))\n",
    "    new_model.add((Dense(input_dim[0])))\n",
    "\n",
    "    # try using different optimizers and different optimizer configs\n",
    "    opt = optimizers.Adam(lr = learning_rate)\n",
    "    model.compile(loss = 'mae', optimizer = opt)\n",
    "    new_model.compile(loss = 'mae', optimizer = opt)\n",
    "\n",
    "    \n",
    "    for i in range(epochNo):\n",
    "        print('epoch:',i+1)\n",
    "        model.fit(x_train, y_train, epochs = 1, batch_size = batch_size, validation_data=(x_test, y_test), \n",
    "                  verbose=1, shuffle=False)\n",
    "        model.reset_states()\n",
    "\n",
    "    \n",
    "    \n",
    "    return model, new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StatefulBiLSTM(x_train, y_train, batch_size, units, dropout, epochNo, learning_rate, x_test, y_test):\n",
    "    \n",
    "    # network\n",
    "    input_dim = (x_train.shape[1],x_train.shape[2])\n",
    "    print((input_dim))\n",
    "    \n",
    "    new_model = Sequential()\n",
    "    model = Sequential()\n",
    "    for i in range(len(units)-1):\n",
    "        #print((units[i]))\n",
    "        model.add(Bidirectional(LSTM(units[i], \n",
    "                       kernel_regularizer=regularizers.l2(0.01), \n",
    "                       recurrent_regularizer=regularizers.l2(0.01), \n",
    "                       bias_regularizer=regularizers.l2(0.01), \n",
    "                       activation='tanh', \n",
    "                        stateful=True, return_sequences=True), batch_input_shape = (batch_size, input_dim[0], input_dim[1])))\n",
    "        \n",
    "        new_model.add(Bidirectional(LSTM(units[i], \n",
    "                       kernel_regularizer=regularizers.l2(0.01), \n",
    "                       recurrent_regularizer=regularizers.l2(0.01), \n",
    "                       bias_regularizer=regularizers.l2(0.01), \n",
    "                       activation='tanh', \n",
    "                        stateful=True, return_sequences=True), batch_input_shape = (1, input_dim[0], input_dim[1])))\n",
    "        \n",
    "        new_model.add(Dropout(dropout))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "\n",
    "    model.add(Bidirectional(LSTM(units[len(units)-1],\n",
    "                   kernel_regularizer=regularizers.l2(0.01), \n",
    "                   recurrent_regularizer=regularizers.l2(0.01), \n",
    "                   bias_regularizer=regularizers.l2(0.01), \n",
    "                   activation='tanh', stateful=True), batch_input_shape = (batch_size, input_dim[0], input_dim[1])))\n",
    "\n",
    "    new_model.add(Bidirectional(LSTM(units[len(units)-1], \n",
    "                   kernel_regularizer=regularizers.l2(0.01), \n",
    "                   recurrent_regularizer=regularizers.l2(0.01), \n",
    "                   bias_regularizer=regularizers.l2(0.01), \n",
    "                   activation='tanh', stateful=True), batch_input_shape = (1, input_dim[0], input_dim[1])))\n",
    "\n",
    "    \n",
    "    \n",
    "    model.add(Dropout(dropout))\n",
    "    model.add((Dense(input_dim[0])))\n",
    "\n",
    "    \n",
    "    new_model.add(Dropout(dropout))\n",
    "    new_model.add((Dense(input_dim[0])))\n",
    "\n",
    "    # try using different optimizers and different optimizer configs\n",
    "    opt = optimizers.Adam(lr = learning_rate)\n",
    "    model.compile(loss = 'mae', optimizer = opt)\n",
    "    new_model.compile(loss = 'mae', optimizer = opt)\n",
    "\n",
    "    \n",
    "    for i in range(epochNo):\n",
    "        print('epoch:',i+1)\n",
    "        model.fit(x_train, y_train, epochs = 1, batch_size = batch_size, validation_data=(x_test, y_test), \n",
    "                  verbose=1, shuffle=False)\n",
    "        model.reset_states()\n",
    "\n",
    "    \n",
    "    \n",
    "    return model, new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## My network\n",
    "def StatelessLSTM(x_train):\n",
    "    input_dim = (x_train.shape[1],x_train.shape[2])\n",
    "    units = 25\n",
    "\n",
    "    model = Sequential()\n",
    "    #model.add(Bidirectional(LSTM(20, return_sequences=True), input_shape = (x_train.shape[1],x_train.shape[2])))\n",
    "    #model.add(Bidirectional(LSTM(20, return_sequences=True))\n",
    "    model.add(LSTM(units, return_sequences=True, input_shape = (x_train.shape[1],x_train.shape[2]),\n",
    "             kernel_regularizer=regularizers.l2(0.01), \n",
    "                   recurrent_regularizer=regularizers.l2(0.01), \n",
    "                                bias_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(LSTM(units, return_sequences=True,kernel_regularizer=regularizers.l2(0.01), \n",
    "                   recurrent_regularizer=regularizers.l2(0.01), \n",
    "                                bias_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(LSTM(units, return_sequences=True,kernel_regularizer=regularizers.l2(0.01), \n",
    "                   recurrent_regularizer=regularizers.l2(0.01), \n",
    "                                bias_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(LSTM(units))\n",
    "\n",
    "    model.add((Dense(50)))\n",
    "\n",
    "    # try using different optimizers and different optimizer configs\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(50, 3)\n",
      "epoch: 1\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 13s 2ms/step - loss: 1.4936 - val_loss: 1.3453\n",
      "epoch: 2\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 988us/step - loss: 1.2590 - val_loss: 1.1388\n",
      "epoch: 3\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 980us/step - loss: 1.0668 - val_loss: 0.9693\n",
      "epoch: 4\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 982us/step - loss: 0.9300 - val_loss: 0.8696\n",
      "epoch: 5\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 1ms/step - loss: 0.8481 - val_loss: 0.8084\n",
      "epoch: 6\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 995us/step - loss: 0.7983 - val_loss: 0.7678\n",
      "epoch: 7\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 7s 1ms/step - loss: 0.7638 - val_loss: 0.7403\n",
      "epoch: 8\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 970us/step - loss: 0.7388 - val_loss: 0.7206\n",
      "epoch: 9\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 950us/step - loss: 0.7225 - val_loss: 0.7066\n",
      "epoch: 10\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 1ms/step - loss: 0.7117 - val_loss: 0.6962\n",
      "epoch: 11\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 961us/step - loss: 0.7023 - val_loss: 0.6883\n",
      "epoch: 12\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 968us/step - loss: 0.6946 - val_loss: 0.6823\n",
      "epoch: 13\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 982us/step - loss: 0.6897 - val_loss: 0.6784\n",
      "epoch: 14\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 954us/step - loss: 0.6849 - val_loss: 0.6744\n",
      "epoch: 15\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 968us/step - loss: 0.6829 - val_loss: 0.6711\n",
      "epoch: 16\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 951us/step - loss: 0.6793 - val_loss: 0.6688\n",
      "epoch: 17\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 948us/step - loss: 0.6762 - val_loss: 0.6667\n",
      "epoch: 18\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 7s 1ms/step - loss: 0.6753 - val_loss: 0.6651\n",
      "epoch: 19\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 942us/step - loss: 0.6727 - val_loss: 0.6633\n",
      "epoch: 20\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 975us/step - loss: 0.6731 - val_loss: 0.6621\n",
      "epoch: 21\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 929us/step - loss: 0.6716 - val_loss: 0.6612\n",
      "epoch: 22\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 989us/step - loss: 0.6684 - val_loss: 0.6602\n",
      "epoch: 23\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 1ms/step - loss: 0.6686 - val_loss: 0.6592\n",
      "epoch: 24\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 7s 1ms/step - loss: 0.6669 - val_loss: 0.6587\n",
      "epoch: 25\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 7s 1ms/step - loss: 0.6671 - val_loss: 0.6576\n",
      "epoch: 26\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 958us/step - loss: 0.6664 - val_loss: 0.6567\n",
      "epoch: 27\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 988us/step - loss: 0.6645 - val_loss: 0.6560\n",
      "epoch: 28\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 1ms/step - loss: 0.6633 - val_loss: 0.6555\n",
      "epoch: 29\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 982us/step - loss: 0.6630 - val_loss: 0.6547\n",
      "epoch: 30\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 7s 1ms/step - loss: 0.6630 - val_loss: 0.6540\n",
      "epoch: 31\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 948us/step - loss: 0.6631 - val_loss: 0.6529\n",
      "epoch: 32\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 967us/step - loss: 0.6622 - val_loss: 0.6523\n",
      "epoch: 33\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 1ms/step - loss: 0.6610 - val_loss: 0.6515\n",
      "epoch: 34\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 967us/step - loss: 0.6605 - val_loss: 0.6509\n",
      "epoch: 35\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 7s 1ms/step - loss: 0.6588 - val_loss: 0.6500\n",
      "epoch: 36\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 975us/step - loss: 0.6586 - val_loss: 0.6502\n",
      "epoch: 37\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 999us/step - loss: 0.6580 - val_loss: 0.6487\n",
      "epoch: 38\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 1ms/step - loss: 0.6590 - val_loss: 0.6477\n",
      "epoch: 39\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 1ms/step - loss: 0.6576 - val_loss: 0.6475\n",
      "epoch: 40\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 985us/step - loss: 0.6570 - val_loss: 0.6470\n",
      "epoch: 41\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 932us/step - loss: 0.6557 - val_loss: 0.6465\n",
      "epoch: 42\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 972us/step - loss: 0.6545 - val_loss: 0.6458\n",
      "epoch: 43\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 988us/step - loss: 0.6550 - val_loss: 0.6453\n",
      "epoch: 44\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 939us/step - loss: 0.6579 - val_loss: 0.6446\n",
      "epoch: 45\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 1ms/step - loss: 0.6560 - val_loss: 0.6442\n",
      "epoch: 46\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 967us/step - loss: 0.6547 - val_loss: 0.6436\n",
      "epoch: 47\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 988us/step - loss: 0.6525 - val_loss: 0.6430\n",
      "epoch: 48\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 964us/step - loss: 0.6535 - val_loss: 0.6426\n",
      "epoch: 49\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 968us/step - loss: 0.6543 - val_loss: 0.6422\n",
      "epoch: 50\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 932us/step - loss: 0.6528 - val_loss: 0.6415\n",
      "epoch: 51\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 919us/step - loss: 0.6530 - val_loss: 0.6412\n",
      "epoch: 52\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6400/6400 [==============================] - 6s 952us/step - loss: 0.6514 - val_loss: 0.6406\n",
      "epoch: 53\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 1ms/step - loss: 0.6507 - val_loss: 0.6403\n",
      "epoch: 54\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 999us/step - loss: 0.6515 - val_loss: 0.6401\n",
      "epoch: 55\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 1ms/step - loss: 0.6485 - val_loss: 0.6396\n",
      "epoch: 56\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 994us/step - loss: 0.6501 - val_loss: 0.6395\n",
      "epoch: 57\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 952us/step - loss: 0.6499 - val_loss: 0.6388\n",
      "epoch: 58\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 1ms/step - loss: 0.6489 - val_loss: 0.6388\n",
      "epoch: 59\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 967us/step - loss: 0.6469 - val_loss: 0.6383\n",
      "epoch: 60\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 969us/step - loss: 0.6523 - val_loss: 0.6380\n",
      "epoch: 61\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 966us/step - loss: 0.6479 - val_loss: 0.6378\n",
      "epoch: 62\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "6400/6400 [==============================] - 6s 971us/step - loss: 0.6478 - val_loss: 0.6376\n",
      "epoch: 63\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      " 640/6400 [==>...........................] - ETA: 5s - loss: 0.6433"
     ]
    }
   ],
   "source": [
    "units = [3, 3, 3]\n",
    "dropout = 0.1\n",
    "batch_size = 80  # a multiple of training set\n",
    "epochNo = 400\n",
    "learning_rate = 0.001\n",
    "print(len(units))\n",
    "\n",
    "model, new_model = StatefulBiLSTM(x_train, y_train, batch_size, units, dropout, epochNo, learning_rate, x_test, y_test)\n",
    "\n",
    "#model, new_model = StatefulLSTM(x_train, y_train, batch_size, units, dropout, epochsNo, learning_rate, x_test, y_test)\n",
    "\n",
    "# Prediction = model.fit(x_train, y_train, epochs=120, batch_size=192, validation_data=(x_test, y_test),\\\n",
    "#                        verbose=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## transfer the statefull LSTM to a proper dimension model \n",
    "\n",
    "opt = optimizers.Adam(lr = learning_rate)\n",
    "old_weights = model.get_weights()\n",
    "new_model.set_weights(old_weights)\n",
    "# compile model\n",
    "new_model.compile(loss = 'mae', optimizer = opt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot history\n",
    "plt.plot(Prediction.history['loss'], label='train')\n",
    "plt.plot(Prediction.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction\n",
    "yhat = new_model.predict(x_test,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 900\n",
    "plt.plot(X[sample, :, 0], y_test[sample,:])\n",
    "plt.plot(X[sample, :, 0], yhat[sample,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
